\documentclass[journal,12pt,onecolumn]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{gensymb}
\usepackage{nopageno}
\usepackage{mathtools}
\usepackage{siunitx}
%\usepackage{graphicx}
\DeclareUnicodeCharacter{2212}{-}
\begin{document}
\centering \textbf{EE608  Adaptive Signal Processing}\\
\medskip
\centering{Problem Set 6}\\
\medskip
\begin{enumerate}
\item \textit{An alternate aspect of the convergence analysis for the RLS algorithm}\\
Suppose there exists a true model for $d(n)$ of the form\\
$$d(n)=X^T(n)W_o+v(n)$$\\
where $v(n)$ is some unknown error, and $W_o$ the true weight vector which is also unknown.\\
\medskip
The question we want to investigate is whether $W(n)\rightarrow {W}_o$ in the mean and the mean square sense. Note this is different from what we did in class where we were considering the convergence of $W(n)$ to $W_*$ the Wiener optimal
solution.\\
\medskip
In this problem we shall only investigate the convergence of $W(n) to W_o$ in the mean. Also consider the RLS
algorithm based on
$$W(n)=[\quad\sum_{k=0}^{n}X(k)X^T(k)]^{-1}[\quad\sum_{k=0}^{n}X(k)d(k)]$$\\
\begin{enumerate}[(a)]
\item Show that, if ${v(i)}\perp\!\!\!\perp{X(k)}$for $k \leq i$, then $E[W(n)] = W_{o}.$\\
\medskip
\item In a deterministic sense, show that $W(n)\rightarrow W_o$ if $\lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n}X(k)v(k)=0.$\\
\end{enumerate}
\item Start with the following non-recursive equations and derive the RLS algorithm having the necessary initial condition $(P(0)=\delta^{-1})I.$\\
$$W(n)=[\delta I+\quad\sum_{i=1}^{n}X(i) X^T(i)]^{-1}[\quad\sum_{i=1}^{n}X(i)d(i)]$$\\
$$P(n)=[\delta I+\quad\sum_{i=1}^{n}X(i)X^T(i)]^{-1}$$\\
\item Simulate the normalized LMS algorithm and compare with the LMS algorithm. For this you should use the 
\begin{enumerate}[(a)]
\item data generated by the model in the previous problem.
\item generate the data for an AR model\\
$$x(n)+a_1{x}{(n-1)}a_2{x}{(n-2)}a_3{x}{(n-3)}+a_4{x}{(n-4)}=v(n)$$\\
where the AR parameters correspond to the four poles $\lambda_1=0.8,\lambda_2=0.6 +j{0.5},\lambda_3=0.6âˆ’j{0.5},\lambda_4= 0.65.$ We
assume that that $v(n)$ is zero mean, unit intensity white sequence. You will need to determine the AR parameters for
the given pole locations. Note, you should generate at least 100 data sets, each data set having large enough points
so that you can get a reasonable assessment of the mean squared error. The length of the data set will depend on
how many iterations are required for convergence\\
\smallskip
Use the following form of the normalized LMS algorithm.
\end{enumerate}
\medskip
$$W(n+1)=W(n)+\frac{aX(n+1)}{c+X^T(n+1)X(n+1)}(d(n)-X^T{(n+1)}W(n)),W(0)=0$$\\
\medskip
You should use the following for the purpose of comparison and validation\\
\begin{enumerate}[(a)]
\medskip
\item Learning curve (i.e mean square error curve)
\item Convergent values of $W(n)$
\item Whiteness of the error
\end{enumerate}
\item Simulate the RLS algorithm using
\begin{enumerate}[(a)]
\item The data generated during the simulations for the LMS and N-LMS algorithms in the earlier home works.
\item Compare the results with those obtained for the LMS and N-LMS algorithms (rate of convergence, excess
mean square error, weight values, etc.)
\end{enumerate}
\medskip
\item Apply the above RLS algorithm for the noise cancellation problem. For this use the data that was used in the earlier
noise cancellation simulation. Compare your noise cancellation results using LMS, NLMS and RLS.
\medskip
\item \textbf{Divergence of RLS Algorithm}
\medskip
\begin{enumerate}[(a)]
\item Simulate the divergence in the RLS algorithm. One way to achieve this is to is to round-off every variable
to, say, the third or second significant digit, at the end of each iteration (if divergence still does not occur,
you may have to round-off to first significant digit).This way, we artificially introduce round-off errors and
examine its accumulation effect.
\smallskip
\item For the the procedure of part (a) which simulates divergence, now implement the reinitialization scheme
for the RLS algorithm and check whether it is able to overcome the problem of divergence. Thus, in the reinitialization algorithm too, you will be rounding off each variable to third or fourth significant digit.
\end{enumerate}
\end{enumerate}
\end{document}