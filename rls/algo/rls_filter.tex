\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{caption}
%\usepackage{multirow}
%\usepackage{multicolumn}
%\usepackage{subcaption}
%\doublespacing
\singlespacing
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage{amssymb}
%\usepackage{graphicx}
\usepackage{newfloat}
%\usepackage{syntax}
\usepackage{listings}
\usepackage{iithtlc}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}



%\usepackage{graphicx}
%\usepackage{amssymb}
%\usepackage{relsize}
%\usepackage[cmex10]{amsmath}
%\usepackage{mathtools}
%\usepackage{amsthm}
%\interdisplaylinepenalty=2500
%\savesymbol{iint}
%\usepackage{txfonts}
%\restoresymbol{TXF}{iint}
%\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{cases}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{enumerate}	
\usepackage{enumitem}
\usepackage{amsmath}
%\usepackage{xtab}
\usepackage{longtable}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}
%\usepackage[framemethod=tikz]{mdframed}
\usepackage{listings}
    %\usepackage[latin1]{inputenc}                                 %%
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
  %optionally (for landscape tables embedded in another document): %%
    \usepackage{lscape}     


\usepackage{url}
\def\UrlBreaks{\do\/\do-}


%\usepackage{stmaryrd}


%\usepackage{wasysym}
%\newcounter{MYtempeqncnt}
\DeclareMathOperator*{\Res}{Res}
%\renewcommand{\baselinestretch}{2}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%\lstset{
%language=C,
%frame=single, 
%breaklines=true
%}

%\lstset{
	%%basicstyle=\small\ttfamily\bfseries,
	%%numberstyle=\small\ttfamily,
	%language=Octave,
	%backgroundcolor=\color{white},
	%%frame=single,
	%%keywordstyle=\bfseries,
	%%breaklines=true,
	%%showstringspaces=false,
	%%xleftmargin=-10mm,
	%%aboveskip=-1mm,
	%%belowskip=0mm
%}

%\surroundwithmdframed[width=\columnwidth]{lstlisting}
\def\inputGnumericTable{}                                 %%
\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
 

\begin{document}
%
\tikzstyle{block} = [rectangle, draw,
    text width=3em, text centered, minimum height=3em]
\tikzstyle{sum} = [draw, circle, node distance=3cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
%\newtheorem{algorithm}{Algorithm}[section]
%\newtheorem{cor}{Corollary}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}

\bibliographystyle{IEEEtran}
%\bibliographystyle{ieeetr}

\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%\numberwithin{equation}{subsection}
\numberwithin{equation}{section}
%\numberwithin{problem}{subsection}
%\numberwithin{definition}{subsection}
\makeatletter
\@addtoreset{figure}{section}
\makeatother

\let\StandardTheFigure\thefigure
%\renewcommand{\thefigure}{\theproblem.\arabic{figure}}
\renewcommand{\thefigure}{\thesection}


%\numberwithin{figure}{subsection}

%\numberwithin{equation}{subsection}
%\numberwithin{equation}{section}
%\numberwithin{equation}{problem}
%\numberwithin{problem}{subsection}
\numberwithin{problem}{section}
%%\numberwithin{definition}{subsection}
%\makeatletter
%\@addtoreset{figure}{problem}
%\makeatother
\makeatletter
\@addtoreset{table}{section}
\makeatother

\let\StandardTheFigure\thefigure
\let\StandardTheTable\thetable
%%\renewcommand{\thefigure}{\theproblem.\arabic{figure}}
%\renewcommand{\thefigure}{\theproblem}

%%\numberwithin{figure}{section}

%%\numberwithin{figure}{subsection}



\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}


\title{ 
	\logo{
Recursive Least Squares Algorithm
	}
}

\author{B Swaroop Reddy and Dr G V V Sharma$^{*}$% <-this % stops a space
	\thanks{*The author is with the Department
		of Electrical Engineering, Indian Institute of Technology, Hyderabad
		502285 India e-mail:  gadepall@iith.ac.in. All content in this manual is released under GNU GPL.  Free and open source.}
	
}	

\maketitle

\tableofcontents
\bigskip

\begin{abstract}
	
	This manual provides an introduction to the Adaptive Recursive Least Squares Algorithm.
	
\end{abstract}

\section{Problem Formulation}
Consider the cost function 
%
\begin{align*}
J(n) &= \sum_{i=1}^{n} \beta (n,i) |e(n)|^2
\end{align*}
%
where
\begin{align}
e(n) &= d(n) - W^{T}(n)X(n)
\\
\text{ and }0 &<\beta (n,i)\leq 1
\end{align}
\begin{problem}
Show that the optimal solution for
%
\begin{equation}
\min_{W} J(n)
\end{equation}
is
\begin{equation}
\label{eq:optW}
W_{*}(n) =\phi ^{-1} (n) z(n)
\end{equation}
where 
\begin{align}
\phi (n) &= \sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)
\\
z(n) &= \sum_{i=1}^{n} \lambda ^{n-i}X(i)d^{T}(i)
%&= \sum_{i=1}^{n-i} \lambda ^{n-i}X(i)X^{T}(i)+X(n)X^{T}(n)
\end{align}
\end{problem}
\solution The optimum value is obtained by solving the following equation
\begin{equation}
\frac{\partial J(n)}{\partial W(n)}=0
\end{equation}
resulting in
\begin{multline}
\sum_{i=1}^{n} \lambda ^{n-i}\lsbrak{0 - X(i)d^{T}(i)-X^{T}(i)d(i)}
\\
+\rsbrak{2W(n))X(i)X^{T}(i)}=0
\end{multline}
which can be expressed as
\begin{align}
\sbrak{\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)}W(n)&=\sum_{i=1}^{n} \lambda ^{n-i}X(i)d^{T}(i)\\
\implies \phi (n) W(n) &=z(n)
\end{align}
\begin{problem}
Show that
\begin{align}
\label{eq:phin}
\phi (n)&=\lambda \phi (n-1) + X(n)X^{T}(n)
\\
z(n)&=\lambda z(n-1) + X(n)X^{T}(n)
\label{eq:zn}
\end{align}
\end{problem}
\section{Update Equations}
\begin{problem}
If
\begin{equation}
A=B^{-1}+CD^{-1}C^{T},
\end{equation}
verify that
\begin{equation}
\label{eq:mat_inv_form}
A^{-1}=B-BC(D+C^{T}BC)^{-1}C^{T}B
\end{equation}
%
through a python script.  
\end{problem}
%
\begin{problem}
Using \eqref{eq:mat_inv_form} and \eqref{eq:phin}, show that
%
\begin{equation}
\label{eq:pn}
P(n)=\lambda ^{-1}\sbrak{I -   \dfrac{\lambda ^{-1}P(n-1)X(n)X^{T}(n)}{1+ \lambda ^{-1}X^{T}(n)P(n-1)X(n)}}P(n-1)
\end{equation}
%\phi ^{-1} (n) &= \lambda ^{-1} \phi ^{-1} (n-1) - \\ & \dfrac{\lambda ^{-2}\phi ^{-1} (n-1)X(n)X^{T}(n)\phi ^{-1} (n-1)}{1+\lambda ^{-1}X^{T}(n)\phi ^{-1} (n-1)X(n)}\\
where 
\begin{equation}
P(n)=\phi ^{-1} (n)
\end{equation} 
%and 
%\begin{equation}
%\label{eq:kn}
%%K(n) = \dfrac{\lambda ^{-1}P(n-1)X(n)}{1+ \lambda ^{-1}X^{T}(n)P(n-1)X(n)}
%\end{equation}
%%
\end{problem}
%
%\begin{problem}
%Using \eqref{eq:kn} show that
%%
%\begin{equation}
%K(n)=P(n)X(n)
%\end{equation}
%\end{problem}
%
\begin{problem}
Show that
\begin{equation}
W(n)= W(n-1)+P(n)X(n)e(n)
\end{equation}
\end{problem}
\section{RLS Algorithm}
\begin{problem}
Obtain an algorithm for getting $e(n)$ from $d(n)$.
\end{problem}
\solution
\begin{enumerate}
\item Initialize the algorithm by setting $P(0) = \delta ^{-1} I$, where $\delta$ is a small positive constant and
$W^{T}(0)=0$. 
\item For $n=1,2,3, \dots$,
 compute the following
 {\small
 \begin{align}
e(n) &= d(n)- X^{T}(n)W(n-1)\\
W(n)&= W(n-1)+P(n)X(n)e(n)\\
P(n)&=\lambda ^{-1}\sbrak{I -\frac{\lambda ^{-1}P(n-1)X(n)X^{T}(n)}{1+ \lambda ^{-1}X^{T}(n)P(n-1)X(n)}}P(n-1)
% K(n) &= \dfrac{\lambda ^{-1}P(n-1)X(n)}{1+ \lambda ^{-1}X^{T}(n)P(n-1)X(n)}\\
%P(n)&=\lambda ^{-1}P(n-1) - \lambda ^{-1} K(n)X^{T}(n)P(n-1)\\
\end{align}
}
\end{enumerate}
\begin{problem}
Download the following script
\begin{lstlisting}
wget https://raw.githubusercontent.com/gadepall/EE5347/master/rls/codes/RLS_NC_SPEECH.py
\end{lstlisting}
and execute it. Compare the output with the LMS output.
\end{problem}
%
\section{Convergence of the RLS Algorithm}
\begin{problem}
Show that the RLS algorithm converges in the mean as well as the mean square sense.
\end{problem}
%\subsection{Convergence in the Mean}
%\begin{problem}
%Show that RLS converges in mean i.e. $E[W(n)]\rightarrow W_{*} $
%\end{problem}
%\solution
%From RLS algorithm
%\begin{equation}
%\hat W(n) =\phi ^{-1} (n) \sum_{i=1}^{n} \lambda ^{n-i}X(i)d^{T}(i)
%\end{equation}
%Where 
%
%$P(n) = \phi^{-1} (n) = [\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)]^{-1}$\\ \\
%$\therefore$ $\hat W(n) =[\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)]^{-1} [\sum_{i=1}^{n} \lambda ^{n-i}X(i)d(i)]$\\
%
%$\delta$ I is added to make sure that inverse exists. \\
%Weiner optimal error is 
%\begin{align*}
%e_{*}(n)&= d(n) - W_{*}X(n)\\
%d(n)&= e_{*}(n) - W_{*}X(n)
%\end{align*} 
%W(n) 
%\begin{align*}
%&=[\delta I +\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)]^{-1} [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n) - W_{*}(n)X(n))]\\
%&=[\delta I +\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)]^{-1} [\delta IW_{*} +\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)^{-1}W_{*}+\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)-\delta IW_{*}]\\
%&=W_{*} + [\delta I +\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)]^{-1}[\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)-\delta IW_{*}]\\
%\end{align*}
%We know that\\
%$R= \dfrac{1}{n}\sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)$\\
%$\phi(n) =\delta I + \sum_{i=1}^{n} \lambda ^{n-i}X(i)X^{T}(i)$\\
%For $\lambda = 1$
%\begin{align*}
%R &= \dfrac{\phi (n)}{n} - \dfrac{\delta I}{n}
%\end{align*}
%\begin{equation}
%[\phi[n]]^{-1} = \dfrac{1}{n}R^{-1}
%\end{equation}
%\begin{equation}
%W(n)=W_{*} + \dfrac{1}{n}R^{-1}\dfrac{1}{n}R^{-1}\Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)-\delta IW_{*}\Big ]\\
%\end{equation}
%\begin{align*}
%E[W(n)]&=E \Bigg [ W_{*} + \dfrac{1}{n}R^{-1}\dfrac{1}{n}R^{-1}\Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)\\ &\hspace{0.5cm}-\delta IW_{*} \Big ]\Bigg ]\\
%&=W_{*} + \dfrac{1}{n}R^{-1}E \Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)\Big ]\\ &\hspace{0.5cm}-\dfrac{1}{n}R^{-1}E[\delta IW_{*}]\\
%&=W_{*} + \dfrac{1}{n}R^{-1}\sum_{i=1}^{n} \lambda ^{n-i}E[X(i)(e_{*}(n)]\\ &\hspace{0.5cm}-\dfrac{1}{n}R^{-1}\delta IW_{*}\\
%&=W_{*} -\dfrac{\delta I}{n}R^{-1}W_{*}
%\end{align*}
%
%\begin{equation}
%\lim_{n \to \infty}E[W(n)] = W_{*}
%\end{equation}
%\subsection{Convergence in Mean-Square sense}
%\begin{problem}
%Using Equation 3.3 Show that RLS converges in mean square sense i.e.
%\begin{equation}
%\lim_{n \to \infty}E[(W(n)-W_{*})(W(n)-W_{*})^{T}]=0 
%\end{equation} 
%\end{problem}
%\solution 
%Let $S(n) = E[(W(n)-W_{*})(W(n)-W_{*})^{T}]$ \\
%From Equation 3.3 \\
%$W(n)- W_{*} = \dfrac{1}{n}R^{-1}\Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)-\delta IW_{*}\Big ]$\\
%S(n)
%\begin{align*}
%&=E \Bigg \{ \Bigg (\dfrac{1}{n}R^{-1}\Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)-\delta IW_{*}\Big ]\Bigg )\\ &\hspace{0.5cm}\Bigg (\dfrac{1}{n}R^{-1}\Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(n)-\delta IW_{*}\Big ]\Bigg )^{T} \Bigg \}\\
%&=E \Bigg \{ \dfrac{\delta ^2 I R^{-1}W_{*}W_{*}^{T}(R^{-1})^{T}}{n^2} -  \dfrac{\delta I}{n^{2}}R^{-1}W_{*} \Big [ \sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(i)\Big ](R^{-1})^{T}\\ &\hspace{0.5cm} - \Bigg ( \dfrac{\delta I}{n^{2}}R^{-1}W_{*} \Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(i)\Big ](R^{-1})^{T}\Bigg )^{T} \\ &\hspace{0.5cm}+ \dfrac{1}{n^2}R^{-1}  \Big (\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(i)\Big )\Big (\sum_{i=1}^{n} \lambda ^{n-i}X(i)e_{*}(i)\Big )^{T}(R^{-1})^{T} \Big )\Bigg \}\\
%&=E \Bigg \{ \dfrac{\delta ^2 I R^{-1}W_{*}W_{*}^{T}(R^{-1})^{T}}{n^2}-  \dfrac{\delta I}{n^{2}}R^{-1}W_{*} \Big [ \sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(i)\Big ](R^{-1})^{T}\\ &\hspace{0.5cm} - \Bigg ( \dfrac{\delta I}{n^{2}}R^{-1}W_{*} \Big [\sum_{i=1}^{n} \lambda ^{n-i}X(i)(e_{*}(i)\Big ](R^{-1})^{T}\Bigg )^{T} \\ &\hspace{0.5cm} + \dfrac{1}{n^2}R^{-1} \Big ( \sum_{i=1}^{n} \sum_{j=1}^{n}\lambda ^{n-i} \lambda ^{n-i}\lambda ^{n-j}X(i)e_{*}(i)e_{*}^{T}(j)X^{T}(j)\Big ) (R^{-1})^{T} \Bigg \}\\ 
%\end{align*}
%Assumptions \\ 
%$X(i) \independent X(j), d(i) \independent d(jd(i) , \independent X(j) $\\
%$e_{*}(i) = d(i) - X^{T}(i)W_{*}$\\
%The 1st term is deterministic and from the above assumptions the Expectations 2nd and 3rd become zero.\\ \\
%$E \Big [X(i)e_{*}(i)e_{*}^{T}(j)X^{T}(j) \Big]$
%\begin{align*}
%&=E[X(i)X^{T}(j)]E[e_{*}(i)e_{*}^{T}(j)]\\
%&=RJ_{min}\\
%S(n)&=\dfrac{\delta ^2 I R^{-1}W_{*}W_{*}^{T}(R^{-1})^{T}}{n^2} + \dfrac{1}{n^2}R^{-1} R J_{min}(R^{-1})^{T}\sum_{i=1}^{n}\Big ( \lambda ^{n-i} \Big)^2\\
%\end{align*}
%\begin{equation}
%\lim_{n \to \infty}S(n) = \lim_{n \to \infty}E  \Big[(W(n)-W_{*})(W(n)-W_{*})^{T}\Big ] = 0
%\end{equation}

\end{document}