|\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{caption}
%\usepackage{multirow}
%\usepackage{multicolumn}
%\usepackage{subcaption}
%\doublespacing
\singlespacing
\usepackage{csvsimple}
\usepackage{amsmath}
\usepackage{multicol}
%\usepackage{enumerate}
\usepackage{amssymb}
%\usepackage{graphicx}
\usepackage{newfloat}
%\usepackage{syntax}
\usepackage{listings}
\usepackage{iithtlc}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}



%\usepackage{graphicx}
%\usepackage{amssymb}
%\usepackage{relsize}
%\usepackage[cmex10]{amsmath}
%\usepackage{mathtools}
%\usepackage{amsthm}
%\interdisplaylinepenalty=2500
%\savesymbol{iint}
%\usepackage{txfonts}
%\restoresymbol{TXF}{iint}
%\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{cases}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{enumerate}	
\usepackage{enumitem}
\usepackage{amsmath}
%\usepackage{xtab}
\usepackage{longtable}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}
%\usepackage[framemethod=tikz]{mdframed}
\usepackage{listings}
    %\usepackage[latin1]{inputenc}                                 %%
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
  %optionally (for landscape tables embedded in another document): %%
    \usepackage{lscape}     


\usepackage{url}
\def\UrlBreaks{\do\/\do-}


%\usepackage{stmaryrd}


%\usepackage{wasysym}
%\newcounter{MYtempeqncnt}
\DeclareMathOperator*{\Res}{Res}
%\renewcommand{\baselinestretch}{2}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%\lstset{
%language=C,
%frame=single, 
%breaklines=true
%}

%\lstset{
	%%basicstyle=\small\ttfamily\bfseries,
	%%numberstyle=\small\ttfamily,
	%language=Octave,
	%backgroundcolor=\color{white},
	%%frame=single,
	%%keywordstyle=\bfseries,
	%%breaklines=true,
	%%showstringspaces=false,
	%%xleftmargin=-10mm,
	%%aboveskip=-1mm,
	%%belowskip=0mm
%}

%\surroundwithmdframed[width=\columnwidth]{lstlisting}
\def\inputGnumericTable{}                                 %%
\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
 

\begin{document}
%
\tikzstyle{block} = [rectangle, draw,
    text width=3em, text centered, minimum height=3em]
\tikzstyle{sum} = [draw, circle, node distance=3cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
%\newtheorem{algorithm}{Algorithm}[section]
%\newtheorem{cor}{Corollary}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}

\bibliographystyle{IEEEtran}
%\bibliographystyle{ieeetr}

\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
%\numberwithin{equation}{subsection}
\numberwithin{equation}{section}
%\numberwithin{problem}{subsection}
%\numberwithin{definition}{subsection}
\makeatletter
\@addtoreset{figure}{section}
\makeatother

\let\StandardTheFigure\thefigure
%\renewcommand{\thefigure}{\theproblem.\arabic{figure}}
\renewcommand{\thefigure}{\thesection}


%\numberwithin{figure}{subsection}

%\numberwithin{equation}{subsection}
%\numberwithin{equation}{section}
%\numberwithin{equation}{problem}
%\numberwithin{problem}{subsection}
\numberwithin{problem}{section}
%%\numberwithin{definition}{subsection}
%\makeatletter
%\@addtoreset{figure}{problem}
%\makeatother
\makeatletter
\@addtoreset{table}{section}
\makeatother

\let\StandardTheFigure\thefigure
\let\StandardTheTable\thetable
%%\renewcommand{\thefigure}{\theproblem.\arabic{figure}}
%\renewcommand{\thefigure}{\theproblem}

%%\numberwithin{figure}{section}

%%\numberwithin{figure}{subsection}



\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}

\title{ 
	\logo{
Least Mean Square Algorithm
	}
}

\author{B Swaroop Reddy and G V V Sharma$^{*}$% <-this % stops a space
	\thanks{*The author is with the Department
		of Electrical Engineering, Indian Institute of Technology, Hyderabad
		502285 India e-mail:  gadepall@iith.ac.in. All content in this manual is released under GNU GPL.  Free and open source.}
	
}	

\maketitle

\tableofcontents

\bigskip

\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}


\begin{abstract}
	
This manual provides an introduction to the LMS algorithm.
		
\end{abstract}
\section{Audio Source Files}
\begin{enumerate}[label=\thesection.\arabic*
,ref=\thesection.\theenumi]
%%
\item Get the \textbf{audio\_source}
\begin{lstlisting}
svn checkout https://github.com/gadepall/EE5347/trunk/audio_source
cd audio_source
\end{lstlisting}
\item Play the \textbf{signal\_noise.wav} and \textbf{noise.wav} file. Comment.
\\
\solution
\textbf{signal\_noise.wav}  contains a human voice along 
with an instrument sound in the background.  This instrument sound
is captured in \textbf{noise.wav}.
\end{enumerate}
%
\section{Problem Formulation}
\begin{enumerate}[label=\thesection.\arabic*
,ref=\thesection.\theenumi]

\item See Table \ref{table:known}.  The goal is to extract the human voice $e(n)$ from $d(n)$ by suppressing the component of $\mbf{X(n)}$.  Formulate 
an equation for this.
\\
\solution The  maximum component of $\mbf{X(n)}$ in $d(n)$ can be estimated as
\begin{equation}
W^{T}(n)X(n)
\end{equation}
		where $e(n)$ is an estimate of the human voice ( desired signal) 
%\begin{itemize}
%\item  
\begin{align}
 X(n)
 =
 %\frac{1}{\det(X)}
  \begin{bmatrix}
   x(n) \\ x(n-1)\\
   x(n-2) \\ $..$ \\ $..$ \\ x(n-M+1)  \end{bmatrix}_{M X 1}
\end{align}
%\item
\begin{align}
 W(n)
 =
 %\frac{1}{\det(X)}
  \begin{bmatrix}
   w_1(n) \\ w_2(n)\\
   w_3(n) \\ $..$ \\ $..$ \\ w_{n-M+1}(n)  \end{bmatrix}_{M X 1}
\end{align}

\begin{align}
\label{eq:error}
e(n) = d(n)-W^{T}(n)X(n)
\end{align}
%

%\section{Solution Strategy}
%
%\csvautotabular{./figs/sigs.csv}
\begin{table}[!ht]
\centering
\small
\input{./figs/sigs.tex}
\caption{}
\label{table:known}
\end{table}
Let
\begin{equation}
d(n) = e(n) + W^{T}(n)X(n)
\end{equation}
%\end{itemize}
and estimating $W(n)$.
The human voice can be characterized as
%
%
The goal is to find $W(n)$ that will allow $W^{T}(n)X(n)$ to mimic the instrument sound in $d(n)$.
This is possible if $e(n)$ is minimum. This problem can be expressed as
\begin{equation}
\label{eq:prob_mse}
\min_{W(n)}e^{2}(n)
\end{equation}
%
\end{enumerate}
\section{LMS Algorithm}
%
%\begin{problem}
%Show that $e^2(n)$ is a convex function.
%\end{problem}
\begin{problem}
Show using \eqref{eq:error}  that 
\begin{align}
\nabla_{W(n)}e^2(n)&=\frac{\partial e^{2}(n)}{\partial W(n)}\\
&=- 2X(n)d(n) + 2 X(n) X^{T}(n)W(n)
\end{align}
%
\end{problem}

\begin{problem}
Use the gradient descent method to obtain an algorithm for solving
\begin{equation}
\label{eq:mse}
\min_{W(n)}e^{2}(n)
\end{equation}
%
\end{problem}
\solution The desired algorithm can be expressed as
%
\begin{align}
W(n+1)&=W(n) - \bar{\mu}[ \nabla_{W(n)}e^2(n)]
\\
W(n+1)&=W(n)+ \mu X(n) e(n)
\end{align}
%
where $\mu = \bar{\mu}$.
\begin{problem}
Write a program to suppress $X(n)$ in $d(n)$.
\end{problem}
\solution Execute \textbf{LMS\_NC\_SPEECH.py}.

\section{Wiener-Hopf Equation}
\begin{problem}
Let
%
\begin{align}
%\label{eq:error}
e(n) = d(n)-W^{T}(n)X(n)
\end{align}
%
Show that
\begin{equation}
E[e^{2}(n)] = r_{dd} - W^{T}(n)r_{xd} - r^{T}_{xd}W(n) + W^{T}(n) R W(n)
\end{equation}
where
\begin{align}
r_{dd} &= E[d^2(n)]
\\
r_{xd} &= E[X(n)d(n)]
\\
R &= E[X(n)X^{T}(n)]
\label{eq:matrix_R}
\end{align}
\end{problem}
%
\begin{problem}
By computing 
\begin{equation}
\frac{\partial J(n)}{\partial W(n)}=0,
\end{equation}
show that the optimal solution for
%
\begin{equation}
\label{eq:wiener_opt}
W^*(n) = \min_{W(n)}E\sbrak{e^2(n)} = R^{-1} r_{xd}
\end{equation}
%
\end{problem}
This is the Wiener optimal solution.
\section{Convergence of the LMS Algorithm}
\subsection{Convergence in the Mean}
\begin{problem}
Show that $R$ in \eqref{eq:matrix_R} is symmetric as well as positive definite.
\end{problem}
%
Let
\begin{equation}
\tilde {W(n)}= W(n) - W_{*}
\end{equation}
%
where $W_{*}$ is obtained in \eqref{eq:wiener_opt}. Also, according to the LMS algorithm,
\begin{align}
\label{eq:wn_update}
W(n+1)&=W(n)+ \mu X(n) e(n)
\\
e(n) &= d(n) - X^{T}(n)W(n)
\end{align}
%
\begin{problem}
%
Show that
\begin{equation}
 E\sbrak{\tilde W(n+1)}=[I - \mu R]E\sbrak{\tilde W(n)}
\end{equation}
\end{problem}
\begin{problem}
Show that 
\begin{equation}
\label{eq:eigen_decompose}
R = U \Lambda U^{T}
\end{equation}
%
for some $U, \Lambda$, such that $\Lambda$ is a diagonal matrix and $U^TU = I$.
\end{problem}
%
\begin{problem}
Show that
\begin{equation}
\label{eq:converge_lambda}
\lim_{n \to \infty}E\sbrak{\tilde W(n+1)} = 0 \iff \lim_{n\to \infty}[I - \mu \Lambda]^n = 0
\end{equation}
\end{problem}
%
\begin{problem}
Using \eqref{eq:converge_lambda}, show that
\begin{equation}
0 < \mu < \frac{2}{\lambda_{\max}}
\end{equation}
%
where $\lambda_{\max}$ is the largest entry of $\Lambda$.
\end{problem}
%
\subsection{Convergence in Mean-square sense}
Let 
\begin{equation}
 X(n)
 =
  \begin{bmatrix}
   X_1(n) \\ X_2(n)  \end{bmatrix}\\
\tilde W(n)
 =
  \begin{bmatrix}
   \tilde W_1(n) \\ \tilde W_2(n) \end{bmatrix}
\end{equation}

\begin{problem}
Show that 
\begin{equation}
E[\tilde W^{T}(n)X(n) X^{T}(n) \tilde W(n)] = E[\tilde W^{T}(n) R \tilde W(n)]
\end{equation}
%
for $R$ defined in \eqref{eq:matrix_R}.
\end{problem}
%\solution
%$LHS = E\Bigg\{ \tilde W^{T}(n)X(n) X^{T}(n) \tilde W(n) \Bigg \}$
%\begin{align*}
%&=E \Bigg \{ \sum_{i=1}^{2} \sum_{j=1}^{2} \tilde W_{i}(n)X_{i}(n) X_{j}(n) \tilde W_j(n) \Bigg \}\\
%&=E \Big \{ \tilde W^{2}_1(n) X^{2}_1(n)+ 2\tilde W_1(n) \tilde W_2(n)X_1(n)X_2(n) + \tilde W^{2}_2(n) X^{2}_2(n) \Big \} \\
%&=E [\tilde W^{2}_1(n)]E[ X^{2}_1(n)]+ 2\tilde E[W_1(n) \tilde W_2(n)]E[X_1(n)X_2(n)]  \\ &\hspace{0.5cm} + E[ \tilde W^{2}_2(n)] E[X^{2}_2(n) ]\\
%&=E \Big \{ \tilde W^{2}_1(n)E[ X^{2}_1(n)]+ 2\tilde W_1(n) \tilde W_2(n)E[X_1(n)X_2(n)]  \\ &\hspace{0.5cm} + \tilde W^{2}_2(n) E[X^{2}_2(n)] \Big \}
%\end{align*}
%\begin{align*}
%R &= E \big [ X(n) X^{T}(n) \big]\\
%&=E \Bigg \{ \begin{bmatrix}
%   X^{2}_1(n) & X_1(n)X_2(n) \\X_1(n)X_2(n) & X^{2}_2(n) \end{bmatrix}
% \Bigg \}\\
%&= \begin{bmatrix}
%   E[X^{2}_1(n)] & E[X_1(n)X_2(n)] \\E[X_1(n)X_2(n)] & E[X^{2}_2(n)] \end{bmatrix} 
%\end{align*}
%$RHS = E \big [\tilde W^{T}(n)R \tilde W(n)\big ]$
%\begin{align*}
%&=E \Big \{ \tilde W^{2}_1(n)E[ X^{2}_1(n)]+ 2\tilde W_1(n) \tilde W_2(n)E[X_1(n)X_2(n)]  \\ &\hspace{0.5cm} + \tilde W^{2}_2(n) E[X^{2}_2(n)] \Big \}
%\end{align*}
%%\begin{problem}

%\begin{problem}
%How can we choose the value of $\mu$ if LMS algorithm converges in mean-square sense. \label{prob2.3}
%\end{problem}
%\solution
%$ 0 < \mu < \dfrac{1}{M(Signal Power)}$
\begin{problem}
Show that 
\begin{multline}
J(n)=E[e^{2}(n)]
=E[e_{*}^{2}(n)]
\\
 + E[\tilde W(n)X(n) X(n)^{T} \tilde W(n)^T] 
- E[ \tilde W(n)X(n)e_{*}(n)] 
\\ 
- E[ e_{*}(n)X^{T}(n)\tilde W^T(n)]
\end{multline}
%
 where  
 \begin{align}
 \label{eq:w_tilde}
 \tilde W(n)&= W(n) - W_{*}\\
 e_{*}(n)&= d(n) - W_{*}X(n)
 \label{eq:e_star}
 \end{align}
\end{problem}
%\solution
%\begin{align*}
%J(n)&=E[e^{2}(n)]\\
%&=E[(d(n) - W^{T}(n) X(n))^{2}]\\
%&=E[(d(n) - W^{T}(n)X(n))^{T}(d(n) - W^{T}(n)X(n))]\\
%&=E[(d(n) - W_{*}^{T} X(n) - W^{T}(n)X(n) + W_{*}^{T} X(n))^{T}\\&(d(n) - W_{*}^{T} X(n)- W^{T}(n)X(n)+ W_{*}^{T} X(n))]\\
%&=E[(e_{*}^{2}(n) - \tilde W(n) X(n))^{T}(e_{*}^{2}(n) - \tilde W(n) X(n))]\\
%\end{align*}
\begin{problem}
Show that
\begin{equation}
E\sbrak{ \tilde W(n)X(n)e_{*}(n)} = E\sbrak{ e_{*}(n)X^{T}(n)\tilde W^T(n)} = 0
\end{equation}
\end{problem}
%
\begin{problem}
Show that
\begin{align}
E\sbrak{\tilde W^{T}(n) R \tilde W(n)} &= \text{trace}\brak{E\sbrak{\tilde W^{T}(n) R \tilde W(n)}}
\\
&= \text{trace}\brak{E\sbrak{\tilde W(n)\tilde W^{T}(n) }R}
\end{align}

\end{problem}
\begin{problem}
Using \eqref{eq:w_tilde}, \eqref{eq:wn_update} and \eqref{eq:e_star},  show that 
\begin{equation}
\tilde W(n+1) = \sbrak{I - \mu X(n) X^{T}(n) }\tilde W(n) + \mu X(n)e_{*}(n)
\end{equation}
\begin{problem}
Let $\mu^2 \to 0$.  Using \eqref{eq:eigen_decompose} and \eqref{eq:wiener_opt}, show that
\begin{equation}
E\sbrak{\tilde W(n+1)\tilde W^{T}(n+1)} = \brak{I - 2\mu R}E\sbrak{\tilde W(n)\tilde W^{n}(n)}
\end{equation}
\end{problem}
%&=\big [I - \mu X(n) X^{T}(n) \big ]\tilde W(n) \\ & \hspace{0.75cm} + \mu X(n)\big [d(n)-X^{T}(n) W_{*}\big ]\\
%&=
%\end{align*}
\end{problem}
%
\begin{problem}
Show that 
\begin{equation}
\lim_{n\to\infty}E\sbrak{\tilde W(n)\tilde W^{T}(n)} = 0 \iff 0 < \mu < \dfrac{1}{\lambda_{max}}
\end{equation}
\end{problem}
%\begin{problem}
%Let $P(n)= E[\tilde W(n)\tilde W^{T}(n)]$ \\and
%$P_{u}(n)=U^{T}P(n)U $. Show that 
%\begin{align*}
%P_{u}(n+1)&=\Big (I - 2\mu \Lambda \Big )P_{u}(n) + \mu ^2J_{min} \Lambda
%\end{align*}
%\end{problem}
%\solution
%From equation 2.8
%\begin{align*}
%\tilde W(n+1)
%&=\big [I - \mu X(n) X^{T}(n) \big ]\tilde W(n) \\ & \hspace{0.75cm} + \mu X(n)\big [d(n)-X^{T}(n) W_{*}\big ]\\
%&=\big [I - \mu X(n) X^{T}(n) \big ]\tilde W(n) + \mu X(n)e_{*}(n)
%\end{align*}
%$E \Big [ \tilde W(n+1)\tilde W^{n}(n+1) \Big ]$
%\begin{align*}
%&=E \Big [\big (I - \mu X(n) X^{T}(n) \big )\tilde W(n)\tilde W^{T}(n)\big (I - \mu X(n) X^{T}(n) \big )^{T} \\ & \hspace{0.75cm} +\mu X(n)e_{*}(n)\tilde W(n)\tilde W^{T}(n)\big (I - \mu X(n) X^{T}(n) \big )^{T} \\ & \hspace{0.75cm} +\big (I - \mu X(n) X^{T}(n) \big )\tilde W(n) \mu e_{*}^{T}(n)X^{T}(n)\\ & \hspace{0.75cm} + \mu ^2  X(n)e_{*}(n)e_{*}^{T}(n)X^{T}(n)\Big ]\\
%&=E \Big [\big (I - \mu X(n) X^{T}(n) \big )\tilde W(n)\tilde W^{T}(n)\big (I - \mu X(n) X^{T}(n) \big )^{T} \Big ]\\ & \hspace{0.75cm} +E\Big [\mu X(n)e_{*}(n)\tilde W(n)\tilde W^{T}(n)\big (I - \mu X(n) X^{T}(n) \big )^{T}\Big ] \\ & \hspace{0.75cm} +E\Big [\big (I - \mu X(n) X^{T}(n) \big )\tilde W(n) \mu e_{*}^{T}(n)X^{T}(n)\Big ]\\ & \hspace{0.75cm} + E\Big [\mu ^2  X(n)e_{*}(n)e_{*}^{T}(n)X^{T}(n)\Big ]
%\end{align*}
%$1^{st}$ term -\\
%$E \Big [\big (I - \mu X(n) X^{T}(n) \big )\tilde W(n)\tilde W^{T}(n)\big (I - \mu X(n) X^{T}(n) \big )^{T} \Big ]$
%\begin{align*}
%&=E\Big [ \Big (\tilde W(n)\tilde W^{T} - \mu X(n) X^{T}(n)\tilde W(n)\tilde W^{T}\Big)\\ & \hspace{0.75cm} \Big (I - \mu X(n) X^{T}(n) \Big )^{T}\Big ]\\
%&=E\Big [ \tilde W(n)\tilde W^{T}(n) - \mu X(n) X^{T}(n)\tilde W(n)\tilde W^{T}(n)\\ & \hspace{0.75cm} - \mu \tilde W(n)\tilde W^{T}(n)\mu X(n) X^{T}(n) \\ & \hspace{0.75cm} + \mu ^2 X(n) X^{T}(n)\tilde W(n)\tilde W^{T}(n) X(n) X^{T}(n)\Big ]\\
%&=E\Big [ \tilde W(n)\tilde W^{T}(n) - 2\mu X(n) X^{T}(n)\tilde W(n)\tilde W^{T}(n)\\ & \hspace{0.75cm} + \mu ^2 X(n) X^{T}(n)\tilde W(n)\tilde W^{T}(n) X(n) X^{T}(n)\Big ]\\
%&=E\Big [ \tilde W(n)\tilde W^{T}(n) - 2\mu X(n) X^{T}(n)\tilde W(n)\tilde W^{T}(n) \Big ] , ( \mu > > 1)\\
%&=E\Big [ \Big (I - 2\mu X(n) X^{T}(n)\Big ) \tilde W(n)\tilde W^{T}(n) \Big ]\\
%&= \Big (I - 2\mu E\big [X(n) X^{T}(n)\big ]\Big ) E\big [\tilde W(n)\tilde W^{T}(n) \big ]\\
%&= \Big (I - 2\mu R \Big ) E\big [\tilde W(n)\tilde W^{T}(n) \big ]
%\end{align*}
%$2^{nd}$ term -\\
%$E\Big [\mu X(n)e_{*}(n)\tilde W(n)\tilde W^{T}(n)\big (I - \mu X(n) X^{T}(n) \big )^{T}\Big ]$
%\begin{align*}
%&=E\Big [\mu X(n)e_{*}(n)\tilde W(n)\tilde W^{T}(n)\\ & \hspace{0.75cm} - \mu ^2 E\Big [\mu X(n)e_{*}(n)\tilde W(n)\tilde W^{T}(n)X(n) X^{T}(n) \Big ]\\
%&=\mu E\Big [ X(n)e_{*}(n) \Big ]E \Big [ \tilde W(n)\tilde W^{T}(n) \Big ]\\
%&=\mu E\Big [ X(n)\big [d(n)-X^{T}(n) W_{*}\big ] \Big ]E \Big [ \tilde W(n)\tilde W^{T}(n) \Big ]\\
%&=\mu \big (r_{xd} - W_{*}R\big ) E \Big [ \tilde W(n)\tilde W^{T}(n) \Big ]\\
%&= 0
%\end{align*}
%Similarly $3^{rd}$ term\\
%$E\Big [\big (I - \mu X(n) X^{T}(n) \big )\tilde W(n) \mu e_{*}^{T}(n)X^{T}(n)\Big ]=0$\\
%
%$4^{th}$ term \\
%$E\Big [\mu ^2  X(n)e_{*}(n)e_{*}^{T}(n)X^{T}(n)\Big ]$
%\begin{align*}
%&=E\Big [\mu ^2  X(n)e^{2}_{*}(n)X^{T}(n)\Big ]\\
%&=\mu ^2E\Big[ e^{2}_{*}(n)\Big] E\Big [ X(n)X^{T}(n)\Big ]\\
%&=\mu ^2J_{min} R\\
%\end{align*}
%Let $R = U \Lambda U^{T}$
%for some $U, \Lambda$,\\ such that $\Lambda$ is a diagonal matrix and $U^TU = I$.\\
%$P_{u}(n+1)$
%\begin{align*}
%&=E \Big [U^{T} \tilde W(n+1)\tilde W^{n}(n+1)U \Big ]+ U^{T}\mu ^2J_{min} RU\\
%&=U^{T}\Big (I - 2\mu R \Big ) E\big [\tilde W(n)\tilde W^{T}(n) \big ]U+ \mu ^2J_{min} U^{T}RU\\
%&=U^{T}\Big (UU^{T} - 2\mu  U \Lambda U^{T} \Big ) E\big [\tilde W(n)\tilde W^{T}(n) \big ]U \\ & \hspace{0.75cm}+ \mu ^2J_{min} U^{T}U \Lambda U^{T}U\\
%&=U^{T}U\Big (I - 2\mu \Lambda \Big ) U^{T}E\big [\tilde W(n)\tilde W^{T}(n) \big ]U + \mu ^2J_{min} \Lambda\\
%&=\Big (I - 2\mu \Lambda \Big )P_{u}(n) + \mu ^2J_{min} \Lambda\\
%\end{align*}

% $E[ e_{*}(n)X^{T}(n)(n)\tilde W(n)]$
%\begin{align*}
% &= E[(d(n) - W_{*}^{T} X(n)X^{T}(n)(n)\tilde W(n))]\\
%&= E[d(n)X^{T}(n)(n)\tilde W(n)] - E[ W_{*}^{T} X(n)X^{T}(n)(n)\tilde W(n)] \\
%&= [r_{xd}- W_{*}^{T}R]\tilde W(n) = 0 
%\end{align*} 
%\begin{align*}
%J(n)&=E[e_{*}^{2}(n)] + E[\tilde W(n)X(n) X(n))^{T} \tilde W(n)]\\
%&= J_{min}(n) + J_{ex}(n)
%\end{align*}

%$E[\tilde W^{T}(n)X(n) X^{T}(n) \tilde W(n)]$
%\begin{align*}
%&=E[\sum_{i=1}^{M} \sum_{j=1}^{M} \tilde W_{i}(n)x_{i}(n) x_{j}(n) \tilde W_j(n)]\\
%&=\sum_{i=1}^{M} \sum_{j=1}^{M} E[\tilde W_{i}(n)x_{i}(n)] E[x_{j}(n) \tilde W_j(n)]\\
%&=E[\sum_{i=1}^{M} \sum_{j=1}^{M} \tilde W_{i}(n) E[x_{i}(n) x_{j}(n)] \tilde W_j(n)]\\
%&=E[\tilde W^{T}(n) R \tilde W(n)]\\
%J(n)&=J_{min} + E[\tilde W^{T}(n) R \tilde W(n)]
%\end{align*}
%Since R is Symmetric and positive semidefinite,
%$ R=U \Lambda U^{T}$ , Where $U = [u_1,u_2,....,u_M]$ and $\lambda_i > 0$ for i =1,2,3,...,M
%\begin{align*}
%J_{ex}(n) &= E[\tilde W^{T}(n) U)(U^{T} \tilde W(n))]\\
%&=\sum_{i=1}^{M} \lambda_i E[\tilde W^{i}_u \tilde W^{i}_u]\\
%&=\sum_{i=1}^{M} \lambda_i p_u^{ii}(n)
%\end{align*}
%$\tilde W(n+1)$
%\begin{align*}
%&=[I - \mu X(n) X^{T}(n)]\tilde W(n) + \mu X(n)[d(n)-X^{T}(n) W_{*}]
%\end{align*} 
%Let $P(n)= E[\tilde W(n)\tilde W^{T}(n)]$ \\and
%$P_u(n)=U^{T}P(n)U $\\
%$p_u^{ii}(n)$ is the $ii^{th}$ diagonal entry of $P_u(n)$\\
%$p_u^{ii}(n+1)= [1-2 \mu \lambda_i]p_u^{ii}(n) + \mu^{2} J_{min} \lambda_i $
%\begin{align*}
%\mid 1-2 \mu \lambda_i \mid < 1\\
%-1 < 1-2 \mu \lambda_i <1 \\
%0 < \mu < \dfrac{1}{\lambda_i}\\
%0 < \mu < \dfrac{1}{\lambda_{max}}< \dfrac{1}{\lambda_i}\\
% 0 < \mu < \dfrac{1}{M(Signal Power)}
%\end{align*}
\begin{problem}
Find the value of the cost function at infinity i.e. $J(\infty)$
\end{problem}
%\solution
%\begin{align*}
%p_u^{ii}(\infty)&= [1-2 \mu \lambda_i]p_u^{ii}(\infty) + \mu^{2} J_{min} \lambda_i \\
%&=\dfrac{\mu J_{min}}{2}\\
%J_{ex}(\infty) &= \sum_{i=1}^{M} \lambda_i p_u^{ii}(\infty)\\
%&=\sum_{i=1}^{M} \lambda_i\dfrac{\mu J_{min}}{2} \\
%&=\dfrac{\mu J_{min}}{2}\sum_{i=1}^{M} \lambda_i \\
%&=\dfrac{\mu J_{min}}{2} tr(R) \\
%&= \dfrac{\mu J_{min}}{2} M(Signal Power) \\
%J(\infty) &= J_{min} + \dfrac{\mu J_{min}}{2}\sum_{i=1}^{M} \lambda_i \\
%\end{align*}
\begin{problem}
How can you choose the value of $\mu$ from the convergence of both in mean and mean-square sense?
\end{problem}

\end{document}
